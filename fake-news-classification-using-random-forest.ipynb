{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6641062,"sourceType":"datasetVersion","datasetId":2093157}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing libraries and observe input folders in Kaggle notebook","metadata":{}},{"cell_type":"code","source":"# Linear algebra\nimport numpy as np\n\n# Data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport subprocess\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\n# Now you can import the NLTK resources as usual\nfrom nltk.corpus import wordnet\n\nimport tensorflow as tf\nprint(\"The TensorFlow version is: \", tf.__version__)\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM,Bidirectional\nfrom tensorflow.keras.layers import Dense, Dropout\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Observe input folders in Kaggle notebook\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:35:35.258492Z","iopub.execute_input":"2024-01-16T15:35:35.259293Z","iopub.status.idle":"2024-01-16T15:35:53.024524Z","shell.execute_reply.started":"2024-01-16T15:35:35.259255Z","shell.execute_reply":"2024-01-16T15:35:53.023561Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Read WELFake_Dataset.csv from kaggle/input","metadata":{}},{"cell_type":"code","source":"# Read WELFake_Dataset.csv from kaggle/input\ndf = pd.read_csv(\"/kaggle/input/fake-news-classification/WELFake_Dataset.csv\")\n# Show first 5 rows of Dataframe\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:35:53.026083Z","iopub.execute_input":"2024-01-16T15:35:53.026678Z","iopub.status.idle":"2024-01-16T15:35:59.025649Z","shell.execute_reply.started":"2024-01-16T15:35:53.026647Z","shell.execute_reply":"2024-01-16T15:35:59.024647Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### EDA(Exploratory Data Analysis)","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:35:59.026917Z","iopub.execute_input":"2024-01-16T15:35:59.027315Z","iopub.status.idle":"2024-01-16T15:35:59.074324Z","shell.execute_reply.started":"2024-01-16T15:35:59.027279Z","shell.execute_reply":"2024-01-16T15:35:59.073428Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Missing data analysis","metadata":{}},{"cell_type":"code","source":"# Missing data analysis\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:35:59.076827Z","iopub.execute_input":"2024-01-16T15:35:59.077121Z","iopub.status.idle":"2024-01-16T15:35:59.099554Z","shell.execute_reply.started":"2024-01-16T15:35:59.077077Z","shell.execute_reply":"2024-01-16T15:35:59.098495Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop NA values\ndf = df.dropna()\n\n# Missing data analysis again\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:35:59.100663Z","iopub.execute_input":"2024-01-16T15:35:59.100967Z","iopub.status.idle":"2024-01-16T15:35:59.145913Z","shell.execute_reply.started":"2024-01-16T15:35:59.100941Z","shell.execute_reply":"2024-01-16T15:35:59.14506Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Drop unused columns","metadata":{}},{"cell_type":"code","source":"df.drop(columns=['Unnamed: 0'],inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:35:59.147148Z","iopub.execute_input":"2024-01-16T15:35:59.147581Z","iopub.status.idle":"2024-01-16T15:35:59.164356Z","shell.execute_reply.started":"2024-01-16T15:35:59.147547Z","shell.execute_reply":"2024-01-16T15:35:59.163312Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Class distribution","metadata":{}},{"cell_type":"code","source":"# Class distribution\n# 0 - Fake, 1 - Real\ndf['label'].value_counts().plot.pie(autopct='%.2f')","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:35:59.165594Z","iopub.execute_input":"2024-01-16T15:35:59.165917Z","iopub.status.idle":"2024-01-16T15:35:59.393392Z","shell.execute_reply.started":"2024-01-16T15:35:59.16589Z","shell.execute_reply":"2024-01-16T15:35:59.391746Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**From the graph we understand that we have balanced data.**","metadata":{}},{"cell_type":"markdown","source":"### Define X and y variables","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns=['label'])\ny = df['label']","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:35:59.395345Z","iopub.execute_input":"2024-01-16T15:35:59.395902Z","iopub.status.idle":"2024-01-16T15:35:59.413463Z","shell.execute_reply.started":"2024-01-16T15:35:59.395851Z","shell.execute_reply":"2024-01-16T15:35:59.412078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vocab_size = Unique words in our Corpus (entire document)\nvocab_size = 10000","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:35:59.41542Z","iopub.execute_input":"2024-01-16T15:35:59.41597Z","iopub.status.idle":"2024-01-16T15:35:59.425145Z","shell.execute_reply.started":"2024-01-16T15:35:59.415926Z","shell.execute_reply":"2024-01-16T15:35:59.423687Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = X.copy()\n\n# We have to reset index as we have used dropna() earlier, otherwise it will throw an error\nmessages.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:35:59.430933Z","iopub.execute_input":"2024-01-16T15:35:59.431717Z","iopub.status.idle":"2024-01-16T15:35:59.443807Z","shell.execute_reply.started":"2024-01-16T15:35:59.431671Z","shell.execute_reply":"2024-01-16T15:35:59.442254Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Lemmatization","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer,WordNetLemmatizer\nfrom nltk.corpus import stopwords\nlm = WordNetLemmatizer()\n\nstopwords = stopwords.words('english')\ncorpus = []\nfor i in range(len(messages)):\n    review = re.sub('^a-zA-Z0-9',' ',messages['title'][i])\n    review = review.lower()\n    review = review.split()\n    review =[lm.lemmatize(x) for x in review if x not in stopwords]\n    review = \" \".join(review)\n    corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:35:59.445801Z","iopub.execute_input":"2024-01-16T15:35:59.446613Z","iopub.status.idle":"2024-01-16T15:36:08.656552Z","shell.execute_reply.started":"2024-01-16T15:35:59.446479Z","shell.execute_reply":"2024-01-16T15:36:08.655659Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_length = max(len(sentence.split()) for sentence in corpus)\n\nprint(\"Maximum sentence length:\", max_length)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:36:08.657702Z","iopub.execute_input":"2024-01-16T15:36:08.657998Z","iopub.status.idle":"2024-01-16T15:36:08.72367Z","shell.execute_reply.started":"2024-01-16T15:36:08.657973Z","shell.execute_reply":"2024-01-16T15:36:08.722682Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(corpus)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:36:08.725156Z","iopub.execute_input":"2024-01-16T15:36:08.725476Z","iopub.status.idle":"2024-01-16T15:36:08.731477Z","shell.execute_reply.started":"2024-01-16T15:36:08.725449Z","shell.execute_reply":"2024-01-16T15:36:08.730511Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### TF-IDF Vectorization","metadata":{}},{"cell_type":"code","source":"tf =TfidfVectorizer()\nx=tf.fit_transform(corpus).toarray()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:36:08.732719Z","iopub.execute_input":"2024-01-16T15:36:08.733017Z","iopub.status.idle":"2024-01-16T15:36:12.985542Z","shell.execute_reply.started":"2024-01-16T15:36:08.732992Z","shell.execute_reply":"2024-01-16T15:36:12.984713Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = df[\"label\"]\ny.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:36:12.986678Z","iopub.execute_input":"2024-01-16T15:36:12.98699Z","iopub.status.idle":"2024-01-16T15:36:12.994068Z","shell.execute_reply.started":"2024-01-16T15:36:12.986963Z","shell.execute_reply":"2024-01-16T15:36:12.993119Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Splitting into train and test sets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:36:12.995171Z","iopub.execute_input":"2024-01-16T15:36:12.995424Z","iopub.status.idle":"2024-01-16T15:36:22.136867Z","shell.execute_reply.started":"2024-01-16T15:36:12.995402Z","shell.execute_reply":"2024-01-16T15:36:22.135698Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:36:22.138254Z","iopub.execute_input":"2024-01-16T15:36:22.138574Z","iopub.status.idle":"2024-01-16T15:58:44.851508Z","shell.execute_reply.started":"2024-01-16T15:36:22.138545Z","shell.execute_reply":"2024-01-16T15:58:44.85028Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Performance Metrics","metadata":{}},{"cell_type":"code","source":"y_pred=rf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:58:44.854333Z","iopub.execute_input":"2024-01-16T15:58:44.854618Z","iopub.status.idle":"2024-01-16T15:58:49.9321Z","shell.execute_reply.started":"2024-01-16T15:58:44.854594Z","shell.execute_reply":"2024-01-16T15:58:49.931155Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\n# Confusion matrix oluştur\ncm = confusion_matrix(y_test, y_pred)\n\n# Matrisi görselleştirme\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, square=True)\nplt.xlabel('Predicted Class')\nplt.ylabel('Real Class')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T15:58:49.933281Z","iopub.execute_input":"2024-01-16T15:58:49.933568Z","iopub.status.idle":"2024-01-16T15:58:50.116436Z","shell.execute_reply.started":"2024-01-16T15:58:49.933542Z","shell.execute_reply":"2024-01-16T15:58:50.115512Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Verilen confusion matrix\nconf_matrix = [[6226, 780],\n               [597, 6705]]\n\n# Hesaplamalar\ntrue_negatives = conf_matrix[0][0]\nfalse_positives = conf_matrix[0][1]\nfalse_negatives = conf_matrix[1][0]\ntrue_positives = conf_matrix[1][1]\n\n# Precision, Recall, F1-score ve Accuracy hesaplamaları\nprecision = true_positives / (true_positives + false_positives)\nrecall = true_positives / (true_positives + false_negatives)\nf1 = 2 * (precision * recall) / (precision + recall)\naccuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n\n# Sonuçları yazdırma\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1-score: {f1:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-01-16T16:02:12.792819Z","iopub.execute_input":"2024-01-16T16:02:12.793285Z","iopub.status.idle":"2024-01-16T16:02:12.802433Z","shell.execute_reply.started":"2024-01-16T16:02:12.793253Z","shell.execute_reply":"2024-01-16T16:02:12.801319Z"},"trusted":true},"outputs":[],"execution_count":null}]}